package service

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"strings"
	"sync"
	"time"
)

// SRTChunkedTranslator x·ª≠ l√Ω translation SRT v·ªõi chunking ƒë·ªÉ t·ªëi ∆∞u t·ªëc ƒë·ªô
type SRTChunkedTranslator struct {
	maxChunkSize    int
	overlapSize     int
	maxConcurrent   int
	timeoutPerChunk time.Duration
	retryAttempts   int
}

// SRTChunk ƒë·∫°i di·ªán cho m·ªôt ph·∫ßn nh·ªè c·ªßa file SRT
type SRTChunk struct {
	ChunkID    int
	StartIndex int
	EndIndex   int
	Content    string
	EntryCount int
	Processed  bool
	Result     string
	Error      error
	RetryCount int
}

// SRTChunkingStrategy chi·∫øn l∆∞·ª£c chia chunk
type SRTChunkingStrategy struct {
	MaxChunkSize    int           // S·ªë c√¢u t·ªëi ƒëa m·ªói chunk (m·∫∑c ƒë·ªãnh: 50)
	OverlapSize     int           // S·ªë c√¢u overlap gi·ªØa c√°c chunk (m·∫∑c ƒë·ªãnh: 0)
	MaxConcurrent   int           // S·ªë chunk x·ª≠ l√Ω ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh: 5)
	TimeoutPerChunk time.Duration // Timeout cho m·ªói chunk (m·∫∑c ƒë·ªãnh: 60s)
	RetryAttempts   int           // S·ªë l·∫ßn retry (m·∫∑c ƒë·ªãnh: 2)
}

// ChunkedTranslationResult k·∫øt qu·∫£ translation v·ªõi chunking
type ChunkedTranslationResult struct {
	TranslatedContent string
	ChunksProcessed   int
	TotalChunks       int
	ProcessingTime    time.Duration
	HasOverlap        bool
}

var (
	chunkedTranslator *SRTChunkedTranslator
	translatorMutex   sync.Mutex
)

// InitSRTChunkedTranslator kh·ªüi t·∫°o SRT Chunked Translator
func InitSRTChunkedTranslator() *SRTChunkedTranslator {
	translatorMutex.Lock()
	defer translatorMutex.Unlock()

	if chunkedTranslator != nil {
		return chunkedTranslator
	}

	chunkedTranslator = &SRTChunkedTranslator{
		maxChunkSize:    50, // 50 c√¢u m·ªói chunk
		overlapSize:     0,  // 0 c√¢u overlap
		maxConcurrent:   5,  // 5 chunks ƒë·ªìng th·ªùi
		timeoutPerChunk: 60 * time.Second,
		retryAttempts:   2,
	}

	log.Printf("SRT Chunked Translator initialized with %d max chunks, %d overlap, %d concurrent",
		chunkedTranslator.maxChunkSize, chunkedTranslator.overlapSize, chunkedTranslator.maxConcurrent)
	return chunkedTranslator
}

// GetSRTChunkedTranslator tr·∫£ v·ªÅ instance c·ªßa SRT Chunked Translator
func GetSRTChunkedTranslator() *SRTChunkedTranslator {
	if chunkedTranslator == nil {
		return InitSRTChunkedTranslator()
	}
	return chunkedTranslator
}

// TranslateSRTWithChunking d·ªãch SRT v·ªõi chunking (h√†m m·ªõi)
// H·ªó tr·ª£ c·∫£ GPT v√† Gemini d·ª±a tr√™n service config
func (t *SRTChunkedTranslator) TranslateSRTWithChunking(
	srtFilePath, apiKey, modelName, targetLanguage string,
	strategy *SRTChunkingStrategy,
) (*ChunkedTranslationResult, error) {
	startTime := time.Now()
	log.Printf("üöÄ [CHUNKED TRANSLATION] B·∫Øt ƒë·∫ßu chunked translation cho %s", srtFilePath)

	// S·ª≠ d·ª•ng strategy m·∫∑c ƒë·ªãnh n·∫øu kh√¥ng c√≥
	if strategy == nil {
		strategy = &SRTChunkingStrategy{
			MaxChunkSize:    t.maxChunkSize,
			OverlapSize:     t.overlapSize,
			MaxConcurrent:   t.maxConcurrent,
			TimeoutPerChunk: t.timeoutPerChunk,
			RetryAttempts:   t.retryAttempts,
		}
	}

	// ƒê·ªçc file SRT
	srtContent, err := os.ReadFile(srtFilePath)
	if err != nil {
		return nil, fmt.Errorf("failed to read SRT file: %v", err)
	}

	// Parse SRT content ƒë·ªÉ ƒë·∫øm s·ªë c√¢u
	entries, err := parseSRT(string(srtContent))
	if err != nil {
		return nil, fmt.Errorf("failed to parse SRT: %v", err)
	}

	totalEntries := len(entries)
	log.Printf("üìä [CHUNKED TRANSLATION] Total SRT entries: %d", totalEntries)

	// N·∫øu √≠t c√¢u h∆°n chunk size, s·ª≠ d·ª•ng translation c≈©
	if totalEntries <= strategy.MaxChunkSize {
		log.Printf("‚ö†Ô∏è [CHUNKED TRANSLATION] SRT ch·ªâ c√≥ %d entries (‚â§ %d), chuy·ªÉn sang TRADITIONAL translation", totalEntries, strategy.MaxChunkSize)

		// T·ª± ƒë·ªông ch·ªçn service d·ª±a tr√™n modelName
		var translatedContent string
		var err error

		if strings.Contains(strings.ToLower(modelName), "gpt") {
			log.Printf("üîß [TRADITIONAL] S·ª≠ d·ª•ng GPT API cho translation")
			translatedContent, err = TranslateSRTWithGPT(srtFilePath, apiKey, modelName, targetLanguage)
		} else {
			log.Printf("üîß [TRADITIONAL] S·ª≠ d·ª•ng Gemini API cho translation")
			// S·ª≠ d·ª•ng Gemini
			translatedContent, err = TranslateSRTFileWithModelAndLanguage(srtFilePath, apiKey, modelName, targetLanguage)
		}

		if err != nil {
			return nil, err
		}

		log.Printf("‚úÖ [TRADITIONAL] Translation ho√†n th√†nh trong %v", time.Since(startTime))
		return &ChunkedTranslationResult{
			TranslatedContent: translatedContent,
			ChunksProcessed:   1,
			TotalChunks:       1,
			ProcessingTime:    time.Since(startTime),
			HasOverlap:        false,
		}, nil
	}

	log.Printf("üéØ [CHUNKED TRANSLATION] SRT c√≥ %d entries (> %d), s·ª≠ d·ª•ng CHUNKED translation", totalEntries, strategy.MaxChunkSize)

	// Chia SRT th√†nh chunks
	chunks, err := t.splitSRTIntoChunks(string(srtContent), strategy)
	if err != nil {
		return nil, fmt.Errorf("failed to split SRT into chunks: %v", err)
	}

	log.Printf("‚úÇÔ∏è [CHUNKED TRANSLATION] ƒê√£ chia SRT th√†nh %d chunks", len(chunks))

	// X·ª≠ l√Ω chunks v·ªõi concurrent processing
	results, err := t.processChunksConcurrent(chunks, apiKey, modelName, targetLanguage, strategy)
	if err != nil {
		return nil, fmt.Errorf("failed to process chunks: %v", err)
	}

	// Gh√©p chunks l·∫°i
	mergedContent, err := t.mergeChunks(results, entries, strategy)
	if err != nil {
		return nil, fmt.Errorf("failed to merge chunks: %v", err)
	}

	// Validate k·∫øt qu·∫£ cu·ªëi c√πng
	if err := t.validateFinalResult(mergedContent, totalEntries); err != nil {
		log.Printf("‚ö†Ô∏è [CHUNKED TRANSLATION] Warning: Final result validation failed: %v", err)
		// Kh√¥ng return error, ch·ªâ log warning
	}

	processingTime := time.Since(startTime)
	log.Printf("üèÅ [CHUNKED TRANSLATION] Chunked translation ho√†n th√†nh trong %v", processingTime)

	return &ChunkedTranslationResult{
		TranslatedContent: mergedContent,
		ChunksProcessed:   len(results),
		TotalChunks:       len(chunks),
		ProcessingTime:    processingTime,
		HasOverlap:        strategy.OverlapSize > 0,
	}, nil
}

// splitSRTIntoChunks chia SRT th√†nh c√°c chunks nh·ªè
func (t *SRTChunkedTranslator) splitSRTIntoChunks(
	srtContent string,
	strategy *SRTChunkingStrategy,
) ([]*SRTChunk, error) {
	entries, err := parseSRT(srtContent)
	if err != nil {
		return nil, err
	}

	var chunks []*SRTChunk
	chunkIndex := 0

	for i := 0; i < len(entries); i += strategy.MaxChunkSize - strategy.OverlapSize {
		endIndex := i + strategy.MaxChunkSize
		if endIndex > len(entries) {
			endIndex = len(entries)
		}

		// T·∫°o chunk content
		chunkContent := t.createChunkContent(entries[i:endIndex])

		chunk := &SRTChunk{
			ChunkID:    chunkIndex,
			StartIndex: i,
			EndIndex:   endIndex,
			Content:    chunkContent,
			EntryCount: endIndex - i,
			Processed:  false,
		}
		chunks = append(chunks, chunk)
		chunkIndex++
	}

	return chunks, nil
}

// createChunkContent t·∫°o n·ªôi dung cho m·ªôt chunk
func (t *SRTChunkedTranslator) createChunkContent(entries []SRTEntry) string {
	var result strings.Builder
	for _, entry := range entries {
		result.WriteString(fmt.Sprintf("%d\n", entry.Index))
		result.WriteString(fmt.Sprintf("%s --> %s\n", formatTime(entry.Start), formatTime(entry.End)))
		result.WriteString(entry.Text + "\n\n")
	}
	return result.String()
}

// processChunksConcurrent x·ª≠ l√Ω chunks v·ªõi concurrent processing
func (t *SRTChunkedTranslator) processChunksConcurrent(
	chunks []*SRTChunk,
	apiKey, modelName, targetLanguage string,
	strategy *SRTChunkingStrategy,
) ([]*SRTChunk, error) {
	log.Printf("üöÄ [CHUNKED TRANSLATION] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω %d chunks v·ªõi concurrent processing (max: %d)", len(chunks), strategy.MaxConcurrent)
	log.Printf("üîß [CHUNKED TRANSLATION] Strategy: chunk_size=%d, overlap=%d, concurrent=%d, timeout=%v",
		strategy.MaxChunkSize, strategy.OverlapSize, strategy.MaxConcurrent, strategy.TimeoutPerChunk)

	var wg sync.WaitGroup
	semaphore := make(chan struct{}, strategy.MaxConcurrent)
	results := make([]*SRTChunk, len(chunks))
	var resultMutex sync.Mutex

	// Kh·ªüi ƒë·ªông workers
	for _, chunk := range chunks {
		wg.Add(1)
		go func(chunk *SRTChunk, index int) {
			defer wg.Done()

			// Acquire semaphore slot
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			log.Printf("üîÑ [CHUNKED TRANSLATION] Worker b·∫Øt ƒë·∫ßu x·ª≠ l√Ω chunk %d (index: %d)", chunk.ChunkID, index)

			// X·ª≠ l√Ω chunk v·ªõi retry logic
			result := t.processSingleChunkWithRetry(chunk, apiKey, modelName, targetLanguage, strategy)

			// L∆∞u k·∫øt qu·∫£ thread-safe
			resultMutex.Lock()
			results[index] = result
			resultMutex.Unlock()

			if result.Error != nil {
				log.Printf("‚ùå [CHUNKED TRANSLATION] Chunk %d failed: %v", chunk.ChunkID, result.Error)
			} else {
				log.Printf("‚úÖ [CHUNKED TRANSLATION] Chunk %d completed successfully", chunk.ChunkID)
			}
		}(chunk, chunk.ChunkID)
	}

	log.Printf("‚è≥ [CHUNKED TRANSLATION] ƒêang ch·ªù t·∫•t c·∫£ %d workers ho√†n th√†nh...", len(chunks))
	wg.Wait()
	log.Printf("üéØ [CHUNKED TRANSLATION] T·∫•t c·∫£ workers ƒë√£ ho√†n th√†nh!")

	// Ki·ªÉm tra l·ªói
	var failedChunks []int
	for i, result := range results {
		if result.Error != nil {
			failedChunks = append(failedChunks, i)
		}
	}

	if len(failedChunks) > 0 {
		log.Printf("‚ö†Ô∏è [CHUNKED TRANSLATION] %d chunks failed: %v", len(failedChunks), failedChunks)

		// Th·ª≠ retry v·ªõi chunk size nh·ªè h∆°n
		log.Printf("üîÑ [CHUNKED TRANSLATION] Th·ª≠ retry failed chunks v·ªõi chunk size nh·ªè h∆°n...")
		if err := t.retryFailedChunksWithSmallerSize(results, failedChunks, apiKey, modelName, targetLanguage, strategy); err != nil {
			return nil, fmt.Errorf("failed to retry failed chunks: %v", err)
		}
		log.Printf("‚úÖ [CHUNKED TRANSLATION] Retry completed")
	}

	log.Printf("üèÅ [CHUNKED TRANSLATION] Concurrent processing ho√†n th√†nh cho %d chunks", len(chunks))
	return results, nil
}

// processSingleChunkWithRetry x·ª≠ l√Ω m·ªôt chunk v·ªõi retry logic
func (t *SRTChunkedTranslator) processSingleChunkWithRetry(
	chunk *SRTChunk,
	apiKey, modelName, targetLanguage string,
	strategy *SRTChunkingStrategy,
) *SRTChunk {
	for attempt := 0; attempt <= strategy.RetryAttempts; attempt++ {
		if attempt > 0 {
			log.Printf("Retrying chunk %d, attempt %d/%d", chunk.ChunkID, attempt, strategy.RetryAttempts)
		}

		// T·∫°o context v·ªõi timeout
		ctx, cancel := context.WithTimeout(context.Background(), strategy.TimeoutPerChunk)

		// X·ª≠ l√Ω chunk
		result, err := t.processSingleChunk(ctx, chunk, apiKey, modelName, targetLanguage)
		cancel()

		if err == nil {
			chunk.Result = result
			chunk.Processed = true
			chunk.Error = nil
			log.Printf("Chunk %d processed successfully", chunk.ChunkID)
			return chunk
		}

		chunk.Error = err
		chunk.RetryCount = attempt

		if attempt < strategy.RetryAttempts {
			// Ch·ªù m·ªôt ch√∫t tr∆∞·ªõc khi retry
			time.Sleep(time.Duration(attempt+1) * time.Second)
		}
	}

	log.Printf("Chunk %d failed after %d attempts: %v", chunk.ChunkID, strategy.RetryAttempts+1, chunk.Error)
	return chunk
}

// processSingleChunk x·ª≠ l√Ω m·ªôt chunk ƒë∆°n l·∫ª
func (t *SRTChunkedTranslator) processSingleChunk(
	ctx context.Context,
	chunk *SRTChunk,
	apiKey, modelName, targetLanguage string,
) (string, error) {
	// T·∫°o prompt cho chunk n√†y
	prompt := t.createChunkPrompt(chunk, targetLanguage)

	// T·ª± ƒë·ªông ch·ªçn service d·ª±a tr√™n modelName
	if strings.Contains(strings.ToLower(modelName), "gpt") {
		return t.callGPTAPI(ctx, prompt, apiKey, modelName)
	} else {
		return t.callGeminiAPI(ctx, prompt, apiKey, modelName)
	}
}

// createChunkPrompt t·∫°o prompt cho chunk
func (t *SRTChunkedTranslator) createChunkPrompt(chunk *SRTChunk, targetLanguage string) string {
	languageMap := map[string]string{
		"vi": "Ti·∫øng Vi·ªát", "en": "Ti·∫øng Anh", "ja": "Ti·∫øng Nh·∫≠t",
		"ko": "Ti·∫øng H√†n", "zh": "Ti·∫øng Trung", "fr": "Ti·∫øng Ph√°p",
		"de": "Ti·∫øng ƒê·ª©c", "es": "Ti·∫øng T√¢y Ban Nha",
	}

	languageName := languageMap[targetLanguage]
	if languageName == "" {
		languageName = "Ti·∫øng Vi·ªát"
	}

	// S·ª≠ d·ª•ng prompt gi·ªëng h·ªát logic c≈© ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch
	return fmt.Sprintf(`H√£y d·ªãch ph·∫ßn SRT sau sang %s, t·ªëi ∆∞u h√≥a ƒë·∫∑c bi·ªát cho Text-to-Speech (TTS).
M·ª•c ti√™u cu·ªëi c√πng l√† b·∫£n d·ªãch khi ƒë∆∞·ª£c ƒë·ªçc l√™n ph·∫£i v·ª´a v·∫∑n m·ªôt c√°ch t·ª± nhi√™n trong kho·∫£ng th·ªùi gian cho ph√©p, ƒë·ªìng th·ªùi ph·∫£n √°nh ƒë√∫ng s·∫Øc th√°i v√† m·ªëi quan h·ªá c·ªßa nh√¢n v·∫≠t qua c√°ch x∆∞ng h√¥.
TU√ÇN TH·ª¶ NGHI√äM NG·∫∂T C√ÅC QUY T·∫ÆC SAU:
QUY T·∫ÆC 1: TIMESTAMP V√Ä S·ªê TH·ª® T·ª∞ L√Ä B·∫§T BI·∫æN
Gi·ªØ nguy√™n 100%% s·ªë th·ª© t·ª± v√† d√≤ng th·ªùi gian (timestamps) t·ª´ file g·ªëc.
TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c thay ƒë·ªïi, l√†m tr√≤n, hay "s·ª≠a l·ªói" th·ªùi gian. ƒê√¢y l√† quy t·∫Øc quan tr·ªçng nh·∫•t.
QUY T·∫ÆC 2: ∆ØU TI√äN H√ÄNG ƒê·∫¶U L√Ä ƒê·ªò D√ÄI C√ÇU D·ªäCH
Ng·∫Øn g·ªçn l√† Vua: C√¢u d·ªãch ph·∫£i ƒë·ªß ng·∫Øn ƒë·ªÉ ƒë·ªçc xong trong kho·∫£ng th·ªùi gian c·ªßa timestamp. ƒê√¢y l√† ∆∞u ti√™n cao h∆°n vi·ªác d·ªãch ƒë·∫ßy ƒë·ªß t·ª´ng ch·ªØ.
Ch·ªß ƒë·ªông c√¥ ƒë·ªçng √Ω: N·∫Øm b·∫Øt √Ω ch√≠nh v√† di·ªÖn ƒë·∫°t l·∫°i m·ªôt c√°ch s√∫c t√≠ch nh·∫•t c√≥ th·ªÉ trong vƒÉn n√≥i. M·∫°nh d·∫°n lo·∫°i b·ªè c√°c t·ª´ ph·ª• kh√¥ng l√†m thay ƒë·ªïi √Ω nghƒ©a c·ªët l√µi.
√Åp d·ª•ng quy t·∫Øc K√Ω t·ª±/Gi√¢y (CPS): C·ªë g·∫Øng gi·ªØ cho c√¢u d·ªãch kh√¥ng v∆∞·ª£t qu√° 17 k√Ω t·ª± cho m·ªói gi√¢y th·ªùi l∆∞·ª£ng.
V√≠ d·ª•: N·∫øu th·ªùi l∆∞·ª£ng l√† 2 gi√¢y, c√¢u d·ªãch n√™n d√†i kho·∫£ng 34 k√Ω t·ª±.
QUY T·∫ÆC 3: D·ªäCH T·ª∞ NHI√äN, LINH HO·∫†T V·ªÄ X∆ØNG H√î
∆Øu ti√™n s·ª± t·ª± nhi√™n v√† ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh giao ti·∫øp.
V·ªÅ c√°ch x∆∞ng h√¥:
ƒê·ªëi v·ªõi ng√¥i th·ª© nh·∫•t, "t√¥i" l√† l·ª±a ch·ªçn m·∫∑c ƒë·ªãnh an to√†n v√† ph·ªï bi·∫øn nh·∫•t trong c√°c t√¨nh hu·ªëng trang tr·ªçng ho·∫∑c v·ªõi ng∆∞·ªùi l·∫°.
Tuy nhi√™n, khi b·ªëi c·∫£nh l√† cu·ªôc tr√≤ chuy·ªán th√¢n m·∫≠t, su·ªìng s√£ gi·ªØa b·∫°n b√®, ng∆∞·ªùi th√¢n ho·∫∑c nh·ªØng ng∆∞·ªùi ngang h√†ng, h√£y ch·ªß ƒë·ªông s·ª≠ d·ª•ng c√°c ƒë·∫°i t·ª´ t·ª± nhi√™n h∆°n nh∆∞ "tao - m√†y", "t·ªõ - c·∫≠u", v.v., ƒë·ªÉ gi·ªØ ƒë∆∞·ª£c s·ª± ch√¢n th·ª±c c·ªßa l·ªùi tho·∫°i.
H·∫°n ch·∫ø s·ª≠ d·ª•ng ƒë·∫°i t·ª´ "ta" tr·ª´ khi b·ªëi c·∫£nh th·∫≠t s·ª± ƒë·∫∑c tr∆∞ng (nh√¢n v·∫≠t l√† vua ch√∫a, th·∫ßn linh, ho·∫∑c c√≥ t√≠nh c√°ch r·∫•t ng·∫°o m·∫°n).
M·ª•c ti√™u l√† l√†m cho l·ªùi tho·∫°i ch√¢n th·ª±c nh∆∞ ng∆∞·ªùi Vi·ªát ƒëang n√≥i chuy·ªán, ch·ª© kh√¥ng ph·∫£i l√† m·ªôt b·∫£n d·ªãch m√°y m√≥c.
QUY T·∫ÆC 4: QU·∫¢ TR·∫¢ V·ªÄ LU√îN L√Ä ƒê·ªäNH D·∫†NG SRT
K·∫øt qu·∫£ tr·∫£ v·ªÅ ch·ªâ l√† n·ªôi dung file srt, kh√¥ng th√™m b·∫•t k·ª≥ 1 ghi ch√∫ hay gi·∫£i th√≠ch g√¨ kh√°c.
QUY T·∫ÆC 5: T√™n nh√¢n v·∫≠t, ho·∫∑c ƒë·ªãa danh. ∆∞u ti√™n ƒë·ªÉ d·∫°ng h√°n vi·ªát, v√≠ d·ª•: Nh·ªã C·∫©u, C√∫c Hoa, ƒê·∫°i Lang, L√£o Tam .... B·∫Øc Kinh, H·ªì Nam, ƒê·∫°i H∆∞ng An Lƒ©nh
QUY T·∫ÆC 6: X·ª¨ L√ù ƒê·∫†I T·ª™ NH√ÇN X∆ØNG C√ì L·ª∞A CH·ªåN
Khi quy t·∫Øc x∆∞ng h√¥ cung c·∫•p m·ªôt l·ª±a ch·ªçn (v√≠ d·ª•: 'th·∫ßy/c√¥', 't√¥i/em' ...), b·∫°n B·∫ÆT BU·ªòC PH·∫¢I CH·ªåN M·ªòT ph∆∞∆°ng √°n ph√π h·ª£p nh·∫•t v·ªõi ng·ªØ c·∫£nh c·ªßa c√¢u tho·∫°i ƒë√≥. TUY·ªÜT ƒê·ªêI KH√îNG ƒë∆∞·ª£c vi·∫øt c·∫£ hai l·ª±a ch·ªçn c√°ch nhau b·∫±ng d·∫•u g·∫°ch ch√©o trong c√¢u d·ªãch.

KI·ªÇM TRA CU·ªêI C√ôNG:
Tr∆∞·ªõc khi xu·∫•t k·∫øt qu·∫£, h√£y t·ª± ki·ªÉm tra l·∫°i ƒë·ªÉ ch·∫Øc ch·∫Øn:
Kh√¥ng c√≥ d√≤ng th·ªùi gian n√†o b·ªã sai l·ªách.
ƒê·ªô d√†i c√¢u d·ªãch h·ª£p l√Ω v·ªõi th·ªùi gian hi·ªÉn th·ªã.
C√°ch x∆∞ng h√¥ ("t√¥i", "tao", "t·ªõ", "m√†y"...) t·ª± nhi√™n v√† ph√π h·ª£p v·ªõi ng·ªØ c·∫£nh c·ªßa ƒëo·∫°n h·ªôi tho·∫°i.
K·∫øt qu·∫£ ch·ªâ lu√¥n l√† n·ªôi dung c·ªßa file srt. Kh√¥ng th√™m b·∫•t k·ª≥ n·ªôi dung ghi ch√∫ hay gi·∫£i th√≠ch n√†o kh√°c

Ph·∫ßn SRT c·∫ßn d·ªãch:
%s`, languageName, chunk.Content)
}

// retryFailedChunksWithSmallerSize retry chunks th·∫•t b·∫°i v·ªõi size nh·ªè h∆°n
func (t *SRTChunkedTranslator) retryFailedChunksWithSmallerSize(
	results []*SRTChunk,
	failedChunkIndices []int,
	apiKey, modelName, targetLanguage string,
	strategy *SRTChunkingStrategy,
) error {
	// Gi·∫£m chunk size cho l·∫ßn retry
	smallerStrategy := *strategy
	smallerStrategy.MaxChunkSize = strategy.MaxChunkSize / 2
	if smallerStrategy.MaxChunkSize < 10 {
		smallerStrategy.MaxChunkSize = 10 // Kh√¥ng nh·ªè h∆°n 10
	}

	log.Printf("Retrying failed chunks with smaller size: %d", smallerStrategy.MaxChunkSize)

	for _, index := range failedChunkIndices {
		chunk := results[index]
		if chunk.Error == nil {
			continue
		}

		// Chia chunk th√†nh chunks nh·ªè h∆°n
		smallerChunks, err := t.splitSRTIntoChunks(chunk.Content, &smallerStrategy)
		if err != nil {
			log.Printf("Failed to split failed chunk %d: %v", chunk.ChunkID, err)
			continue
		}

		// X·ª≠ l√Ω chunks nh·ªè h∆°n
		smallerResults, err := t.processChunksConcurrent(smallerChunks, apiKey, modelName, targetLanguage, &smallerStrategy)
		if err != nil {
			log.Printf("Failed to process smaller chunks for failed chunk %d: %v", chunk.ChunkID, err)
			continue
		}

		// Gh√©p chunks nh·ªè h∆°n l·∫°i
		mergedContent, err := t.mergeChunks(smallerResults, nil, &smallerStrategy)
		if err != nil {
			log.Printf("Failed to merge smaller chunks for failed chunk %d: %v", chunk.ChunkID, err)
			continue
		}

		// C·∫≠p nh·∫≠t k·∫øt qu·∫£
		chunk.Result = mergedContent
		chunk.Processed = true
		chunk.Error = nil
		log.Printf("Successfully retried failed chunk %d with smaller size", chunk.ChunkID)
	}

	return nil
}

// mergeChunks gh√©p c√°c chunks l·∫°i th√†nh SRT ho√†n ch·ªânh
func (t *SRTChunkedTranslator) mergeChunks(
	chunks []*SRTChunk,
	originalEntries []SRTEntry,
	strategy *SRTChunkingStrategy,
) (string, error) {
	var mergedEntries []SRTEntry
	var overlapMap map[int]bool

	if strategy.OverlapSize > 0 {
		overlapMap = make(map[int]bool)
	}

	for i, chunk := range chunks {
		if !chunk.Processed {
			return "", fmt.Errorf("chunk %d not processed: %v", chunk.ChunkID, chunk.Error)
		}

		// Parse k·∫øt qu·∫£ chunk
		chunkEntries, err := parseSRT(chunk.Result)
		if err != nil {
			return "", fmt.Errorf("failed to parse chunk %d result: %v", chunk.ChunkID, err)
		}

		// X·ª≠ l√Ω overlap v·ªõi chunk tr∆∞·ªõc (ch·ªâ khi c√≥ overlap)
		if i > 0 && strategy.OverlapSize > 0 {
			chunkEntries = t.handleOverlap(chunkEntries, chunks[i-1], overlapMap)
		}

		mergedEntries = append(mergedEntries, chunkEntries...)
	}

	// T·∫°o SRT content
	return t.createSRTFromEntries(mergedEntries), nil
}

// handleOverlap x·ª≠ l√Ω overlap gi·ªØa c√°c chunks
func (t *SRTChunkedTranslator) handleOverlap(
	currentEntries []SRTEntry,
	previousChunk *SRTChunk,
	overlapMap map[int]bool,
) []SRTEntry {
	if !previousChunk.Processed || len(currentEntries) == 0 {
		return currentEntries
	}

	// Parse previous chunk result
	previousEntries, err := parseSRT(previousChunk.Result)
	if err != nil {
		log.Printf("Warning: failed to parse previous chunk for overlap handling: %v", err)
		return currentEntries
	}

	// T√¨m entries overlap
	var filteredEntries []SRTEntry
	for _, entry := range currentEntries {
		// Ki·ªÉm tra xem entry n√†y c√≥ b·ªã overlap kh√¥ng
		isOverlap := false
		for _, prevEntry := range previousEntries {
			if entry.Index == prevEntry.Index {
				isOverlap = true
				overlapMap[entry.Index] = true
				break
			}
		}

		if !isOverlap {
			filteredEntries = append(filteredEntries, entry)
		}
	}

	return filteredEntries
}

// createSRTFromEntries t·∫°o SRT content t·ª´ entries
func (t *SRTChunkedTranslator) createSRTFromEntries(entries []SRTEntry) string {
	var result strings.Builder
	for i, entry := range entries {
		result.WriteString(fmt.Sprintf("%d\n", i+1))
		result.WriteString(fmt.Sprintf("%s --> %s\n", formatTime(entry.Start), formatTime(entry.End)))
		result.WriteString(entry.Text + "\n\n")
	}
	return result.String()
}

// validateFinalResult validate k·∫øt qu·∫£ cu·ªëi c√πng
func (t *SRTChunkedTranslator) validateFinalResult(mergedContent string, expectedEntryCount int) error {
	entries, err := parseSRT(mergedContent)
	if err != nil {
		return fmt.Errorf("failed to parse merged content: %v", err)
	}

	actualCount := len(entries)
	if abs(actualCount-expectedEntryCount) > 5 {
		return fmt.Errorf("entry count mismatch: expected %d, got %d", expectedEntryCount, actualCount)
	}

	// Ki·ªÉm tra timing continuity
	if err := t.validateTimingContinuity(entries); err != nil {
		return fmt.Errorf("timing continuity validation failed: %v", err)
	}

	return nil
}

// validateTimingContinuity ki·ªÉm tra t√≠nh li√™n t·ª•c c·ªßa timing
func (t *SRTChunkedTranslator) validateTimingContinuity(entries []SRTEntry) error {
	if len(entries) < 2 {
		return nil
	}

	for i := 1; i < len(entries); i++ {
		prevEnd := entries[i-1].End
		currStart := entries[i].Start

		// Ki·ªÉm tra xem timing c√≥ h·ª£p l√Ω kh√¥ng
		if prevEnd >= currStart {
			log.Printf("Warning: timing issue at entry %d: prev end %f >= curr start %f",
				i, prevEnd, currStart)
		}
	}

	return nil
}

// abs helper function
func abs(x int) int {
	if x < 0 {
		return -x
	}
	return x
}

// callGPTAPI g·ªçi GPT API v·ªõi context timeout
func (t *SRTChunkedTranslator) callGPTAPI(ctx context.Context, prompt, apiKey, modelName string) (string, error) {
	// G·ªçi GPT API
	reqBody := GPTRequest{
		Model: modelName,
		Messages: []GPTMessage{
			{Role: "user", Content: prompt},
		},
	}

	// G·ªçi API v·ªõi context timeout
	url := "https://api.openai.com/v1/chat/completions"
	bodyBytes, _ := json.Marshal(reqBody)
	req, _ := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(bodyBytes))
	req.Header.Set("Authorization", "Bearer "+apiKey)
	req.Header.Set("Content-Type", "application/json")

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return "", fmt.Errorf("API call failed: %v", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("API returned status %d", resp.StatusCode)
	}

	respBody, _ := io.ReadAll(resp.Body)
	var gptResp GPTResponse
	json.Unmarshal(respBody, &gptResp)

	if len(gptResp.Choices) > 0 {
		translatedContent := gptResp.Choices[0].Message.Content

		// Clean up response gi·ªëng h·ªát logic c≈©
		translatedContent = t.cleanupGPTResponse(translatedContent)

		return translatedContent, nil
	}

	return "", fmt.Errorf("no response from API")
}

// callGeminiAPI g·ªçi Gemini API v·ªõi context timeout
func (t *SRTChunkedTranslator) callGeminiAPI(ctx context.Context, prompt, apiKey, modelName string) (string, error) {
	// G·ªçi Gemini API
	translatedContent, err := GenerateWithGemini(prompt, apiKey, modelName)
	if err != nil {
		return "", fmt.Errorf("Gemini API call failed: %v", err)
	}

	// Clean up response gi·ªëng h·ªát logic c≈©
	translatedContent = t.cleanupGeminiResponse(translatedContent)

	return translatedContent, nil
}

// cleanupGPTResponse clean up GPT response gi·ªëng h·ªát logic c≈©
func (t *SRTChunkedTranslator) cleanupGPTResponse(translatedContent string) string {
	// Clean up the response - remove any extra text that might be added by GPT
	translatedContent = strings.TrimSpace(translatedContent)

	// Remove markdown code blocks if present
	translatedContent = strings.TrimPrefix(translatedContent, "```")
	translatedContent = strings.TrimSuffix(translatedContent, "```")
	translatedContent = strings.TrimSpace(translatedContent)

	// If GPT added any prefix or explanation, try to extract just the SRT content
	if strings.Contains(translatedContent, "1\n") {
		// Find the start of the SRT content
		startIndex := strings.Index(translatedContent, "1\n")
		if startIndex != -1 {
			translatedContent = translatedContent[startIndex:]
		}
	}

	// Remove any explanatory text that might be added after the SRT content
	endMarkers := []string{
		"\n\n**Gi·∫£i th√≠ch",
		"\n\nGi·∫£i th√≠ch",
		"\n\n**",
		"\n\nT√¥i ƒë√£",
		"\n\nHy v·ªçng",
		"\n\nB·∫£n d·ªãch",
		"\n\n---",
		"\n\nNote:",
		"\n\nL∆∞u √Ω:",
		"\n\n```",
		"```",
	}

	for _, marker := range endMarkers {
		if index := strings.Index(translatedContent, marker); index != -1 {
			translatedContent = strings.TrimSpace(translatedContent[:index])
			break
		}
	}

	// Final cleanup - ensure we only have valid SRT content
	lines := strings.Split(translatedContent, "\n")
	var cleanLines []string
	inSRTContent := false

	for _, line := range lines {
		line = strings.TrimSpace(line)

		// Start SRT content when we see a number
		if t.isNumeric(line) {
			inSRTContent = true
		}

		// Stop if we see explanatory text
		if inSRTContent && (strings.Contains(line, "**") ||
			strings.Contains(line, "Gi·∫£i th√≠ch") ||
			strings.Contains(line, "T√¥i ƒë√£") ||
			strings.Contains(line, "Hy v·ªçng") ||
			strings.Contains(line, "B·∫£n d·ªãch") ||
			strings.Contains(line, "Note:") ||
			strings.Contains(line, "L∆∞u √Ω:") ||
			strings.Contains(line, "```")) {
			break
		}

		if inSRTContent {
			cleanLines = append(cleanLines, line)
		}
	}

	if len(cleanLines) > 0 {
		translatedContent = strings.Join(cleanLines, "\n")
	}

	return translatedContent
}

// cleanupGeminiResponse clean up Gemini response gi·ªëng h·ªát logic c≈©
func (t *SRTChunkedTranslator) cleanupGeminiResponse(translatedContent string) string {
	// Clean up response gi·ªëng logic c≈©
	translatedContent = strings.TrimSpace(translatedContent)

	// N·∫øu Gemini th√™m prefix, extract ch·ªâ SRT content
	if strings.Contains(translatedContent, "1\n") {
		startIndex := strings.Index(translatedContent, "1\n")
		if startIndex != -1 {
			translatedContent = translatedContent[startIndex:]
		}
	}

	// Remove any explanatory text that might be added after the SRT content
	endMarkers := []string{
		"\n\n**Gi·∫£i th√≠ch",
		"\n\nGi·∫£i th√≠ch",
		"\n\n**",
		"\n\nT√¥i ƒë√£",
		"\n\nHy v·ªçng",
		"\n\nB·∫£n d·ªãch",
		"\n\n---",
		"\n\nNote:",
		"\n\nL∆∞u √Ω:",
		"\n\n```",
		"```",
	}

	for _, marker := range endMarkers {
		if index := strings.Index(translatedContent, marker); index != -1 {
			translatedContent = strings.TrimSpace(translatedContent[:index])
			break
		}
	}

	// Final cleanup - ensure we only have valid SRT content
	lines := strings.Split(translatedContent, "\n")
	var cleanLines []string
	inSRTContent := false

	for _, line := range lines {
		line = strings.TrimSpace(line)

		// Start SRT content when we see a number
		if t.isNumeric(line) {
			inSRTContent = true
		}

		// Stop if we see explanatory text
		if inSRTContent && (strings.Contains(line, "**") ||
			strings.Contains(line, "Gi·∫£i th√≠ch") ||
			strings.Contains(line, "T√¥i ƒë√£") ||
			strings.Contains(line, "Hy v·ªçng") ||
			strings.Contains(line, "B·∫£n d·ªãch") ||
			strings.Contains(line, "Note:") ||
			strings.Contains(line, "L∆∞u √Ω:") ||
			strings.Contains(line, "```")) {
			break
		}

		if inSRTContent {
			cleanLines = append(cleanLines, line)
		}
	}

	if len(cleanLines) > 0 {
		translatedContent = strings.Join(cleanLines, "\n")
	}

	return translatedContent
}

// isNumeric helper function
func (t *SRTChunkedTranslator) isNumeric(s string) bool {
	for _, r := range s {
		if r < '0' || r > '9' {
			return false
		}
	}
	return len(s) > 0
}

// TranslateSRTWithChunkingWrapper wrapper function ƒë·ªÉ t√≠ch h·ª£p v·ªõi logic c≈©
// H·ªó tr·ª£ c·∫£ GPT v√† Gemini d·ª±a tr√™n service config
func TranslateSRTWithChunkingWrapper(srtFilePath, apiKey, modelName, targetLanguage string) (string, error) {
	// Kh·ªüi t·∫°o chunked translator
	translator := GetSRTChunkedTranslator()

	// S·ª≠ d·ª•ng strategy m·∫∑c ƒë·ªãnh
	strategy := &SRTChunkingStrategy{
		MaxChunkSize:    50, // 50 c√¢u m·ªói chunk
		OverlapSize:     0,  // 0 c√¢u overlap
		MaxConcurrent:   5,  // 5 chunks ƒë·ªìng th·ªùi
		TimeoutPerChunk: 60 * time.Second,
		RetryAttempts:   2,
	}

	// G·ªçi chunked translation
	result, err := translator.TranslateSRTWithChunking(srtFilePath, apiKey, modelName, targetLanguage, strategy)
	if err != nil {
		return "", err
	}

	return result.TranslatedContent, nil
}

// TranslateSRTWithContextAwareness wrapper function m·ªõi v·ªõi context awareness
// H·ªó tr·ª£ c·∫£ GPT v√† Gemini d·ª±a tr√™n service config
func TranslateSRTWithContextAwareness(srtFilePath, apiKey, modelName, targetLanguage string) (string, error) {
	log.Printf("üöÄ [CONTEXT AWARE TRANSLATION] B·∫Øt ƒë·∫ßu context-aware translation cho %s", srtFilePath)

	// B∆∞·ªõc 1: Ph√¢n t√≠ch ng·ªØ c·∫£nh (m·ªôt l·∫ßn g·ªçi API duy nh·∫•t)
	contextAnalyzer := NewContextAnalyzer(apiKey, modelName)
	contextResult, err := contextAnalyzer.AnalyzeSRTContext(srtFilePath, targetLanguage)
	if err != nil {
		log.Printf("‚ö†Ô∏è [CONTEXT AWARE TRANSLATION] Context analysis failed, fallback to chunked translation: %v", err)
		// Fallback to chunked translation n·∫øu context analysis th·∫•t b·∫°i
		return TranslateSRTWithChunkingWrapper(srtFilePath, apiKey, modelName, targetLanguage)
	}

	// B∆∞·ªõc 2: T·∫°o prompt m·∫´u v·ªõi context awareness
	contextAwarePrompt := contextAnalyzer.GenerateContextAwarePrompt(contextResult, targetLanguage)

	// B∆∞·ªõc 3: Chia file SRT th√†nh chunks
	chunks, err := SplitSRTIntoChunksForContextAware(srtFilePath, 50, 0) // 50 c√¢u m·ªói chunk, 0 overlap
	if err != nil {
		return "", fmt.Errorf("failed to split SRT into chunks: %v", err)
	}

	log.Printf("üìä [CONTEXT AWARE TRANSLATION] ƒê√£ chia SRT th√†nh %d chunks", len(chunks))

	// B∆∞·ªõc 4: X·ª≠ l√Ω chunks v·ªõi context-aware prompts
	results, err := processChunksWithContextAwareness(chunks, contextAwarePrompt, apiKey, modelName, 5) // 5 concurrent
	if err != nil {
		return "", fmt.Errorf("failed to process chunks with context awareness: %v", err)
	}

	// B∆∞·ªõc 5: Gh√©p chunks l·∫°i
	mergedContent, err := mergeChunksWithContextAwareness(results, chunks)
	if err != nil {
		return "", fmt.Errorf("failed to merge chunks: %v", err)
	}

	log.Printf("‚úÖ [CONTEXT AWARE TRANSLATION] Context-aware translation ho√†n th√†nh!")
	return mergedContent, nil
}

// SplitSRTIntoChunksForContextAware chia SRT th√†nh chunks cho context-aware translation
func SplitSRTIntoChunksForContextAware(srtFilePath string, maxChunkSize, overlapSize int) ([]*SRTChunk, error) {
	// T·∫°o temporary translator ƒë·ªÉ s·ª≠ d·ª•ng createChunkContent method
	tempTranslator := &SRTChunkedTranslator{}
	srtContent, err := os.ReadFile(srtFilePath)
	if err != nil {
		return nil, err
	}

	entries, err := parseSRT(string(srtContent))
	if err != nil {
		return nil, err
	}

	var chunks []*SRTChunk
	chunkIndex := 0

	for i := 0; i < len(entries); i += maxChunkSize - overlapSize {
		endIndex := i + maxChunkSize
		if endIndex > len(entries) {
			endIndex = len(entries)
		}

		chunkContent := tempTranslator.createChunkContent(entries[i:endIndex])

		chunk := &SRTChunk{
			ChunkID:    chunkIndex,
			StartIndex: i,
			EndIndex:   endIndex,
			Content:    chunkContent,
			EntryCount: endIndex - i,
			Processed:  false,
		}
		chunks = append(chunks, chunk)
		chunkIndex++
	}

	return chunks, nil
}

// processChunksWithContextAwareness x·ª≠ l√Ω chunks v·ªõi context-aware prompts
func processChunksWithContextAwareness(chunks []*SRTChunk, contextAwarePrompt, apiKey, modelName string, maxConcurrent int) ([]*SRTChunk, error) {
	log.Printf("üöÄ [CONTEXT AWARE TRANSLATION] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω %d chunks v·ªõi context awareness (max concurrent: %d)", len(chunks), maxConcurrent)

	var wg sync.WaitGroup
	semaphore := make(chan struct{}, maxConcurrent)
	results := make([]*SRTChunk, len(chunks))
	var resultMutex sync.Mutex

	// Kh·ªüi ƒë·ªông workers
	for _, chunk := range chunks {
		wg.Add(1)
		go func(chunk *SRTChunk, index int) {
			defer wg.Done()

			// Acquire semaphore slot
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			log.Printf("üîÑ [CONTEXT AWARE TRANSLATION] Worker b·∫Øt ƒë·∫ßu x·ª≠ l√Ω chunk %d (index: %d)", chunk.ChunkID, index)

			// T·∫°o prompt cho chunk n√†y v·ªõi context awareness
			chunkPrompt := strings.Replace(contextAwarePrompt, "{{SRT_CONTENT}}", chunk.Content, 1)

			// X·ª≠ l√Ω chunk v·ªõi context-aware prompt
			result := processSingleChunkWithContextAwareness(chunk, chunkPrompt, apiKey, modelName)

			// L∆∞u k·∫øt qu·∫£ thread-safe
			resultMutex.Lock()
			results[index] = result
			resultMutex.Unlock()

			if result.Error != nil {
				log.Printf("‚ùå [CONTEXT AWARE TRANSLATION] Chunk %d failed: %v", chunk.ChunkID, result.Error)
			} else {
				log.Printf("‚úÖ [CONTEXT AWARE TRANSLATION] Chunk %d completed successfully", chunk.ChunkID)
			}
		}(chunk, chunk.ChunkID)
	}

	log.Printf("‚è≥ [CONTEXT AWARE TRANSLATION] ƒêang ch·ªù t·∫•t c·∫£ %d workers ho√†n th√†nh...", len(chunks))
	wg.Wait()
	log.Printf("üéØ [CONTEXT AWARE TRANSLATION] T·∫•t c·∫£ workers ƒë√£ ho√†n th√†nh!")

	return results, nil
}

// processSingleChunkWithContextAwareness x·ª≠ l√Ω m·ªôt chunk v·ªõi context-aware prompt
func processSingleChunkWithContextAwareness(chunk *SRTChunk, chunkPrompt, apiKey, modelName string) *SRTChunk {
	// T·ª± ƒë·ªông ch·ªçn service d·ª±a tr√™n modelName
	var translatedContent string
	var err error

	if strings.Contains(strings.ToLower(modelName), "gpt") {
		translatedContent, err = callGPTAPIForChunk(chunkPrompt, apiKey, modelName)
	} else {
		translatedContent, err = callGeminiAPIForChunk(chunkPrompt, apiKey, modelName)
	}

	if err != nil {
		chunk.Error = err
		chunk.Processed = false
	} else {
		chunk.Result = translatedContent
		chunk.Processed = true
		chunk.Error = nil
	}

	return chunk
}

// callGPTAPIForChunk g·ªçi GPT API cho m·ªôt chunk
func callGPTAPIForChunk(prompt, apiKey, modelName string) (string, error) {
	url := "https://api.openai.com/v1/chat/completions"

	requestBody := map[string]interface{}{
		"model":       modelName,
		"messages":    []map[string]string{{"role": "user", "content": prompt}},
		"temperature": 0.1,
		"max_tokens":  4000,
	}

	jsonData, err := json.Marshal(requestBody)
	if err != nil {
		return "", fmt.Errorf("failed to marshal request: %v", err)
	}

	req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return "", fmt.Errorf("failed to create request: %v", err)
	}

	req.Header.Set("Authorization", "Bearer "+apiKey)
	req.Header.Set("Content-Type", "application/json")

	ctx, cancel := context.WithTimeout(context.Background(), 60*time.Second)
	defer cancel()
	req = req.WithContext(ctx)

	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		return "", fmt.Errorf("GPT API request failed: %v", err)
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", fmt.Errorf("failed to read response: %v", err)
	}

	if resp.StatusCode != http.StatusOK {
		return "", fmt.Errorf("GPT API error: %s - %s", resp.Status, string(body))
	}

	var response struct {
		Choices []struct {
			Message struct {
				Content string `json:"content"`
			} `json:"message"`
		} `json:"choices"`
	}

	if err := json.Unmarshal(body, &response); err != nil {
		return "", fmt.Errorf("failed to parse GPT response: %v", err)
	}

	if len(response.Choices) == 0 {
		return "", fmt.Errorf("no choices in GPT response")
	}

	translatedContent := response.Choices[0].Message.Content

	// Remove markdown code blocks if present - simple approach
	translatedContent = strings.TrimPrefix(translatedContent, "```srt")
	translatedContent = strings.TrimPrefix(translatedContent, "```")
	translatedContent = strings.TrimSuffix(translatedContent, "```")
	// translatedContent = strings.TrimSpace(translatedContent)

	// Remove any remaining "srt" at the beginning
	if strings.HasPrefix(translatedContent, "srt") {
		translatedContent = strings.TrimPrefix(translatedContent, "srt")
		//translatedContent = strings.TrimSpace(translatedContent)
	}

	return translatedContent, nil
}

// callGeminiAPIForChunk g·ªçi Gemini API cho m·ªôt chunk
func callGeminiAPIForChunk(prompt, apiKey, modelName string) (string, error) {
	// S·ª≠ d·ª•ng service Gemini c√≥ s·∫µn
	return GenerateWithGemini(prompt, apiKey, modelName)
}

// mergeChunksWithContextAwareness gh√©p chunks l·∫°i v·ªõi context awareness
func mergeChunksWithContextAwareness(results []*SRTChunk, originalChunks []*SRTChunk) (string, error) {
	var mergedBuilder strings.Builder
	var seenEntries map[int]bool = make(map[int]bool)
	var entryCounter int = 1

	for _, result := range results {
		if result.Error != nil {
			return "", fmt.Errorf("chunk %d failed: %v", result.ChunkID, result.Error)
		}

		if !result.Processed {
			return "", fmt.Errorf("chunk %d not processed", result.ChunkID)
		}

		// Parse chunk result ƒë·ªÉ x·ª≠ l√Ω overlap
		chunkEntries, err := parseSRT(result.Result)
		if err != nil {
			return "", fmt.Errorf("failed to parse chunk %d result: %v", result.ChunkID, err)
		}

		// X·ª≠ l√Ω t·ª´ng entry trong chunk
		for _, entry := range chunkEntries {
			// Ki·ªÉm tra xem entry n√†y ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ch∆∞a (d·ª±a tr√™n index)
			if !seenEntries[entry.Index] {
				seenEntries[entry.Index] = true

				// Ghi entry v·ªõi s·ªë th·ª© t·ª± m·ªõi
				mergedBuilder.WriteString(fmt.Sprintf("%d\n", entryCounter))
				mergedBuilder.WriteString(fmt.Sprintf("%s --> %s\n", formatTime(entry.Start), formatTime(entry.End)))
				mergedBuilder.WriteString(entry.Text + "\n\n")

				entryCounter++
			}
		}
	}

	return mergedBuilder.String(), nil
}
